# TRANSFORMER-MODEL-FOR-CODE-GENERATION

NLP,
Distributed Training, Deployment | GitHub
Personal Project | 2024
• Developed and trained a Transformer-based model for code generation
using PyTorch on a custom dataset, leveraging distributed training (DDP)
across 4 GPUs.
• Implemented mixed-precision training (AMP), accelerating training cycles by
nearly 3x while maintaining model convergence.
• Fine-tuned model for Python code snippet generation, achieving >80%
functional accuracy on representative test cases.
• Deployed model as a REST API endpoint using TensorFlow Serving and
Docker for real-time inference capabilities.
