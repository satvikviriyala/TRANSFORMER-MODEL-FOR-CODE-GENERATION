# Model Configuration
model:
  architecture: "transformer"
  vocab_size: 32000
  hidden_size: 768
  num_layers: 12
  num_heads: 12
  max_position_embeddings: 2048
  dropout: 0.1
  activation_function: "gelu"

# Training Configuration
training:
  batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000
  max_steps: 100000
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  fp16: true
  gradient_checkpointing: true

# LoRA Configuration
lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_proj", "v_proj"]

# Data Configuration
data:
  train_file: "data/train.jsonl"
  eval_file: "data/eval.jsonl"
  max_seq_length: 2048
  preprocessing_num_workers: 4

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  mlflow:
    tracking_uri: "http://localhost:5000"
    experiment_name: "code_generation"

# Serving Configuration
serving:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  timeout: 60
  max_batch_size: 32
  max_sequence_length: 2048 